"""ReAct loop implementation for X-Agent.

The ReAct (Reasoning + Acting) loop enables the agent to:
1. Think about what to do next
2. Decide whether to use a tool
3. Execute the tool if needed
4. Observe the result
5. Reflect on the outcome and adjust strategy
6. Repeat until done

This creates an iterative problem-solving capability with self-reflection.
"""

import json
import re
from collections.abc import AsyncGenerator
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable

from ..services.llm.router import LLMRouter
from ..tools.base import BaseTool, ToolResult
from ..tools.manager import ToolManager
from .plan_context import PlanState  # Add PlanState type
from ..utils.logger import get_logger

# âœ… OPTIMIZE: Import error learning service for memory integration
try:
    from ..services.error_learning import get_error_learning_service
    ERROR_LEARNING_AVAILABLE = True
except ImportError:
    ERROR_LEARNING_AVAILABLE = False
    logger.warning("Error learning service not available, memory integration disabled")

logger = get_logger(__name__)

__all__ = [
    # Event types
    "REACT_EVENT_THINKING",
    "REACT_EVENT_TOOL_CALL",
    "REACT_EVENT_TOOL_RESULT",
    "REACT_EVENT_CHUNK",
    "REACT_EVENT_FINAL",
    "REACT_EVENT_ERROR",
    "REACT_EVENT_REFLECTION",
    "REACT_EVENT_STRATEGY_ADJUSTMENT",
    # Data classes
    "ReflectionType",
    "ReflectionResult",
    "ReflectionRecord",
    "ToolExecutionRecord",
    "StrategyState",
    "ToolCallRequest",
    # Main class
    "ReActLoop",
]


# ReAct event types
REACT_EVENT_THINKING = "thinking"
REACT_EVENT_TOOL_CALL = "tool_call"
REACT_EVENT_TOOL_RESULT = "tool_result"
REACT_EVENT_CHUNK = "chunk"
REACT_EVENT_FINAL = "final_answer"
REACT_EVENT_ERROR = "error"
REACT_EVENT_REFLECTION = "reflection"  # NEW: Reflection event
REACT_EVENT_STRATEGY_ADJUSTMENT = "strategy_adjustment"  # NEW: Strategy adjustment event


class ReflectionType(str, Enum):
    """Types of reflection in the ReAct loop.
    
    Implements 5 reflection scenarios:
    1. TOOL_RESULT: Result anomaly (error, empty, format mismatch, low confidence)
    2. CHECKPOINT: Stage goal reached (milestone validation)
    3. PRE_ACTION: Before high-risk actions (delete, write, send, paid API)
    4. ADAPTIVE: Multiple failures (retry strategy adjustment)
    5. LONG_TASK: Periodic check during long execution (prevent drift)
    """
    # Scenario 1: Result anomaly (MUST reflect)
    TOOL_RESULT = "tool_result"  # Reflect on tool execution result
    ERROR_DRIVEN = "error_driven"  # API error, empty result, schema mismatch
    
    # Scenario 2: Checkpoint reflection
    CHECKPOINT = "checkpoint"  # After reaching stage goal
    MILESTONE_REACHED = "milestone_reached"  # Milestone validation passed
    
    # Scenario 3: Pre-action reflection (high-risk)
    PRE_ACTION = "pre_action"  # Before risky operations
    HIGH_RISK_DECISION = "high_risk_decision"  # Delete, write, send, execute
    
    # Scenario 4: Adaptive reflection (multiple failures)
    ADAPTIVE = "adaptive"  # After repeated failures
    STRATEGY_ADJUSTMENT = "strategy_adjustment"  # Adjust approach
    
    # Scenario 5: Long task rhythm control
    LONG_TASK = "long_task"  # Periodic check during execution
    PROGRESS_CHECK = "progress_check"  # Prevent task drift
    
    # Legacy types (backward compatibility)
    PLAN_ADJUSTMENT = "plan_adjustment"  # Reflect and adjust plan
    FAILURE_ANALYSIS = "failure_analysis"  # Analyze failure and suggest recovery
    FINAL_VERIFICATION = "final_verification"  # Verify final answer completeness


@dataclass
class ReflectionResult:
    """Result of a reflection operation.
    
    Attributes:
        should_adjust: Whether strategy adjustment is needed
        reason: Explanation of the reflection
        suggestion: Suggested adjustment or correction
        confidence: Confidence level (0.0-1.0)
        adjusted_plan: Optional adjusted plan
    """
    should_adjust: bool
    reason: str
    suggestion: str
    confidence: float = 0.5
    adjusted_plan: str | None = None


@dataclass
class ToolExecutionRecord:
    """Record of a tool execution for pattern analysis.
    
    Attributes:
        tool_name: Name of the tool
        arguments: Tool arguments
        success: Whether execution succeeded
        output: Execution output
        error: Error message if failed
        iteration: Iteration number when executed
    """
    tool_name: str
    arguments: dict[str, Any]
    success: bool
    output: str
    error: str | None
    iteration: int


@dataclass
class ReflectionRecord:
    """Record of a reflection event.
    
    Attributes:
        iteration: Iteration number when reflection occurred
        reflection_type: Type of reflection
        reason: Explanation of the reflection
        suggestion: Suggested adjustment
        timestamp: When the reflection occurred
    """
    iteration: int
    reflection_type: str
    reason: str
    suggestion: str
    timestamp: float = field(default_factory=lambda: __import__('time').time())


@dataclass
class StrategyState:
    """Track strategy state for adaptive behavior.
    
    Attributes:
        consecutive_failures: Number of consecutive tool failures
        same_tool_repeated: Count of same tool being called repeatedly
        last_tool_name: Name of last executed tool
        adjustment_count: Number of strategy adjustments made
        failed_tool_patterns: Set of tools that have failed
        reflection_history: List of past reflections
    """
    consecutive_failures: int = 0
    same_tool_repeated: int = 0
    last_tool_name: str | None = None
    adjustment_count: int = 0
    failed_tool_patterns: set[str] = field(default_factory=set)
    reflection_history: list[ReflectionRecord] = field(default_factory=list)
    tool_execution_history: list[ToolExecutionRecord] = field(default_factory=list)


@dataclass
class ToolCallRequest:
    """A request to call a tool from the LLM.
    
    Attributes:
        id: Unique ID for this tool call
        name: Name of the tool to call
        arguments: Arguments to pass to the tool
    """
    id: str
    name: str
    arguments: dict[str, Any]


class ReActLoop:
    """ReAct loop for iterative reasoning and action with self-reflection.
    
    Implements the ReAct pattern with enhanced self-reflection:
    - Reason about what to do
    - Act by calling tools
    - Observe the results
    - Reflect on outcomes and adjust strategy
    - Repeat until task is complete
    
    Example:
        loop = ReActLoop(llm_router, tool_manager)
        
        # Streaming mode
        async for event in loop.run_streaming(messages):
            if event["type"] == "thinking":
                print(f"Thinking: {event['content']}")
            elif event["type"] == "tool_call":
                print(f"Calling tool: {event['name']}")
            elif event["type"] == "reflection":
                print(f"Reflection: {event['content']}")
            elif event["type"] == "final_answer":
                print(f"Answer: {event['content']}")
    """
    
    MAX_ITERATIONS = 8  # Maximum ReAct iterations
    MAX_CONSECUTIVE_FAILURES = 2  # Max failures before strategy adjustment
    MAX_SAME_TOOL_REPEATS = 2  # Max repeats of same tool before suggesting alternative
    MAX_ADJUSTMENTS = 3  # Max strategy adjustments to prevent infinite loops
    MAX_RETRY_WITHOUT_TOOL = 2  # Max retries when LLM doesn't call tools (iteration < 2è§„èŒƒ)
    
    def __init__(
        self,
        llm_router: LLMRouter,
        tool_manager: ToolManager,
        max_iterations: int = 5,  # âœ… OPTIMIZE: Reduced from 8 to 5 for faster failure detection
        enable_reflection: bool = True,  # NEW: Enable/disable reflection
    ) -> None:
        """Initialize the ReAct loop.
        
        Args:
            llm_router: LLM router for making API calls
            tool_manager: Tool manager for executing tools
            max_iterations: Maximum number of iterations
            enable_reflection: Whether to enable self-reflection capabilities
        """
        self.llm_router = llm_router
        self.tool_manager = tool_manager
        self.max_iterations = max_iterations
        self.enable_reflection = enable_reflection
        
        # ğŸ”¥ NEW: SKILL.md content cache (avoid repeated file reads)
        self._skill_md_cache: dict[str, str] = {}
        
        logger.debug(  # Changed from info: initialization is routine
            "ReActLoop initialized with progressive skill disclosure",
            extra={
                "max_iterations": max_iterations,
                "enable_reflection": enable_reflection,
            }
        )
    
    def _extract_skill_guidance_for_execution(self, tool_name: str, arguments: dict) -> str:
        """ä» SKILL.md ä¸­æå–æ‰§è¡Œæ—¶çš„å…³é”®æŒ‡å¼•ï¼ˆæ¸è¿›å¼æŠ«éœ²ï¼‰
        
        Args:
            tool_name: å·¥å…·åç§°ï¼ˆå¦‚ run_in_terminalï¼‰
            arguments: å·¥å…·å‚æ•°
            
        Returns:
            æå–çš„å…³é”®æŒ‡å¼•æ–‡æœ¬
        """
        import re
        from pathlib import Path
        
        # ğŸ”¥ æ£€æµ‹æ˜¯å¦æ¶‰åŠæŠ€èƒ½è„šæœ¬æ‰§è¡Œ
        script_name = None
        skill_name = None
        
        if tool_name == "run_in_terminal":
            command = arguments.get("command", "")
            
            # ä»å‘½ä»¤ä¸­æå–è„šæœ¬å
            # ç¤ºä¾‹ï¼špython create_pdf_from_md.py ... â†’ create_pdf_from_md.py
            # node create_presentation.js ... â†’ create_presentation.js
            parts = command.split()
            for i, part in enumerate(parts):
                if part in ["python", "python3", "node", "npm", "yarn"] and i + 1 < len(parts):
                    script_candidate = parts[i + 1]
                    # æå–è„šæœ¬æ–‡ä»¶å
                    script_name = script_candidate.split("/")[-1].split("\\")[-1]
                    break
        
        if not script_name:
            return ""
        
        # ğŸ”¥ æ ¹æ®è„šæœ¬åæ¨æ–­æŠ€èƒ½ç±»å‹
        if "pdf" in script_name.lower():
            skill_name = "pdf"
        elif "ppt" in script_name.lower() or "presentation" in script_name.lower():
            skill_name = "pptx"
        elif "xlsx" in script_name.lower() or "excel" in script_name.lower():
            skill_name = "xlsx"
        
        if not skill_name:
            return ""
        
        # æ£€æŸ¥ç¼“å­˜
        if skill_name in self._skill_md_cache:
            skill_md_content = self._skill_md_cache[skill_name]
        else:
            # è¯»å– SKILL.md æ–‡ä»¶
            try:
                skill_dir = Path(__file__).parent.parent / 'skills' / skill_name
                skill_md_path = skill_dir / 'SKILL.md'
                
                if not skill_md_path.exists():
                    logger.warning(f"SKILL.md not found for skill: {skill_name}")
                    return ""
                
                skill_md_content = skill_md_path.read_text(encoding='utf-8')
                self._skill_md_cache[skill_name] = skill_md_content
                logger.info(
                    f"Loaded SKILL.md for {skill_name} ({len(skill_md_content)} chars)",
                    extra={"skill": skill_name, "tool": tool_name}
                )
            except Exception as e:
                logger.error(f"Failed to read SKILL.md: {e}")
                return ""
        
        # ğŸ”¥ æ¸è¿›å¼æŠ«éœ²ç­–ç•¥ï¼š
        # 1. è¯†åˆ«å½“å‰æ‰§è¡Œçš„è„šæœ¬ç±»å‹
        # 2. æå–ç›¸å…³çš„å‘½ä»¤æ ¼å¼å’Œç¤ºä¾‹
        # 3. åªè¿”å›æœ€å…³é”®çš„ä¿¡æ¯ï¼ˆæ­£ç¡®ç”¨æ³• + å¸¸è§é”™è¯¯ï¼‰
        
        guidance_parts = []
        
        # === PPTX æŠ€èƒ½æ‰§è¡ŒæŒ‡å¼• ===
        if skill_name == "pptx" and ("presentation" in script_name.lower() or "ppt" in script_name.lower()):
            guidance_parts.append("\n\n## âš ï¸ PPTX è„šæœ¬æ‰§è¡Œå…³é”®æŒ‡å¼•")
            guidance_parts.append("- âœ… **å”¯ä¸€æŒ‡å®šè„šæœ¬**: `create_presentation.js`")
            guidance_parts.append("- âœ… **æ­£ç¡®å‘½ä»¤æ ¼å¼**: `node create_presentation.js <input.md> <output.pptx>`")
            guidance_parts.append("- âœ… **ç¤ºä¾‹**: `node create_presentation.js /workspace/report.md /workspace/presentation.pptx`")
            guidance_parts.append("- âŒ **ç¦æ­¢**: ä½¿ç”¨ Python è„šæœ¬æˆ–ä¸å­˜åœ¨çš„è„šæœ¬")
            guidance_parts.append("- ğŸ’¡ **è·¯å¾„è§„åˆ™**: ç›´æ¥ä½¿ç”¨è„šæœ¬åï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨æŸ¥æ‰¾ï¼‰")
            
            # ä» SKILL.md ä¸­æå– Usage éƒ¨åˆ†
            usage_match = re.search(r'### Usage[\s\S]*?(?=###|## $)', skill_md_content)
            if usage_match:
                guidance_parts.append("\n\n## ğŸ“– SKILL.md å®˜æ–¹ç”¨æ³•")
                guidance_parts.append(usage_match.group(0)[:500])  # é™åˆ¶é•¿åº¦
            
            logger.info(
                "Extracted PPTX execution guidance",
                extra={
                    "skill": skill_name,
                    "script_name": script_name,
                    "guidance_length": len("\n".join(guidance_parts)),
                }
            )
            return "\n".join(guidance_parts)
        
        # === PDF æŠ€èƒ½æ‰§è¡ŒæŒ‡å¼• ===
        elif skill_name == "pdf" and "pdf" in script_name.lower():
            guidance_parts.append("\n\n## ğŸŒ PDF è„šæœ¬æ‰§è¡Œå…³é”®æŒ‡å¼•")
            guidance_parts.append("- âœ… **å”¯ä¸€æŒ‡å®šè„šæœ¬**: `create_pdf_from_md.py`ï¼ˆå¢å¼ºç‰ˆï¼‰")
            guidance_parts.append("- âœ… **æ­£ç¡®å‘½ä»¤æ ¼å¼**: `python create_pdf_from_md.py output.pdf input.md \"æ ‡é¢˜\"`")
            guidance_parts.append("- âœ… **å·²æ³¨å†Œä¸­æ–‡å­—ä½“**: PingFang/STHeiti")
            guidance_parts.append("- âœ… **æ”¯æŒå¤šé¡µã€è‡ªåŠ¨åˆ†é¡µã€ç« èŠ‚æ ¼å¼åŒ–**")
            guidance_parts.append("- âŒ **ç¦æ­¢**: ä½¿ç”¨ä¸å­˜åœ¨çš„è„šæœ¬æˆ–æ—§ç‰ˆè„šæœ¬")
            guidance_parts.append("- ğŸ’¡ **è·¯å¾„è§„åˆ™**: ç›´æ¥ä½¿ç”¨è„šæœ¬åï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨æŸ¥æ‰¾ï¼‰")
            
            logger.info(
                "Extracted PDF execution guidance",
                extra={
                    "skill": skill_name,
                    "script_name": script_name,
                    "guidance_length": len("\n".join(guidance_parts)),
                }
            )
            return "\n".join(guidance_parts)
        
        # === XLSX æŠ€èƒ½æ‰§è¡ŒæŒ‡å¼• ===
        elif skill_name == "xlsx" and ("xlsx" in script_name.lower() or "excel" in script_name.lower()):
            guidance_parts.append("\n\n## ğŸ“Š XLSX è„šæœ¬æ‰§è¡Œå…³é”®æŒ‡å¼•")
            guidance_parts.append("- âœ… **ä½¿ç”¨ Node.js + ExcelJS**")
            guidance_parts.append("- âœ… **æ­£ç¡®å‘½ä»¤æ ¼å¼**: `node create_spreadsheet.js <input.json> <output.xlsx>`")
            guidance_parts.append("- âœ… **æ”¯æŒä¸­æ–‡ã€å…¬å¼ã€æ ¼å¼åŒ–**")
            guidance_parts.append("- âŒ **ç¦æ­¢**: ä½¿ç”¨ä¸å­˜åœ¨çš„ Python è„šæœ¬")
            guidance_parts.append("- ğŸ’¡ **è·¯å¾„è§„åˆ™**: ç›´æ¥ä½¿ç”¨è„šæœ¬åï¼ˆç³»ç»Ÿä¼šè‡ªåŠ¨æŸ¥æ‰¾ï¼‰")
            
            logger.info(
                "Extracted XLSX execution guidance",
                extra={
                    "skill": skill_name,
                    "script_name": script_name,
                    "guidance_length": len("\n".join(guidance_parts)),
                }
            )
            return "\n".join(guidance_parts)
        
        return ""
    
    async def run(
        self,
        messages: list[dict[str, str]],
        tools: list[BaseTool] | None = None,
    ) -> str:
        """Run the ReAct loop (non-streaming).
        
        Args:
            messages: Conversation messages
            tools: Available tools (uses tool_manager if not provided)
            
        Returns:
            Final response string
        """
        result = ""
        async for event in self.run_streaming(messages, tools):
            if event.get("type") == REACT_EVENT_FINAL:
                result = event.get("content", "")
            elif event.get("type") == REACT_EVENT_ERROR:
                result = f"Error: {event.get('error', 'Unknown error')}"
        
        return result
    
    async def run_streaming(
        self,
        messages: list[dict[str, str]],
        tools: list[BaseTool] | None = None,
        session_id: str | None = None,
        skill_context: Any = None,  # Phase 2 - Skill metadata for tool restrictions
        plan_state: PlanState | None = None,  # NEW: PlanState for structured plan tool constraints
    ) -> AsyncGenerator[dict[str, Any], None]:
        """Run the ReAct loop with streaming events.
        
        Yields events as the loop progresses:
        - thinking: LLM is reasoning
        - tool_call: A tool is being called
        - tool_result: Result from tool execution
        - chunk: Response chunk from LLM
        - final_answer: Final response
        - error: An error occurred
        
        Args:
            messages: Conversation messages
            tools: Available tools (uses tool_manager if not provided)
            session_id: Optional session ID for logging
            skill_context: SkillMetadata object (Phase 2, for tool restrictions)
            plan_state: PlanState object (for structured plan tool constraints)
            
        Yields:
            Event dictionaries
        """
        # Working message list
        working_messages = list(messages)
        
        # Use provided tools or get from manager
        if tools is None:
            tools = self.tool_manager.get_all_tools()
        
        # Apply plan tool constraints if structured plan exists
        if plan_state and hasattr(plan_state, 'structured_plan') and plan_state.structured_plan:
            plan = plan_state.structured_plan
            if hasattr(plan, 'tool_constraints') and plan.tool_constraints:
                # âœ… FIX: Use Plan's original tool constraints directly, don't re-compute
                original_count = len(tools)
                tools = [
                    t for t in tools
                    if plan.tool_constraints.is_allowed(t.name)
                ]
                filtered_count = original_count - len(tools)
                                
                logger.info(
                    "Applied Plan's original tool constraints (highest priority)",
                    extra={
                        "original_tools": original_count,
                        "allowed_tools": len(tools),
                        "filtered_out": filtered_count,
                        "constraint_source": plan.tool_constraints.source,
                        "constraint_priority": plan.tool_constraints.priority,
                        "allowed_list": plan.tool_constraints.allowed if plan.tool_constraints.allowed else "all",
                        "forbidden_list": plan.tool_constraints.forbidden if plan.tool_constraints.forbidden else "none",
                    }
                )
        
                # Emit Info event for user visibility
                if filtered_count > 0:
                    constraint_msg = []
                    if plan.tool_constraints.allowed:
                        constraint_msg.append(f"âœ… ä»…å…è®¸ï¼š{', '.join(plan.tool_constraints.allowed)}")
                    if plan.tool_constraints.forbidden:
                        constraint_msg.append(f"âŒ ç¦æ­¢ï¼š{', '.join(plan.tool_constraints.forbidden)}")
        
                    yield {
                        "type": "message",
                        "role": "system",
                        "content": f"ğŸ”§ **å·¥å…·çº¦æŸå·²åº”ç”¨**\n\n{chr(10).join(constraint_msg)}\n\nå·²è¿‡æ»¤ {filtered_count} ä¸ªå·¥å…·",
                    }
        
        # ğŸ”¥ NEW: Add plan execution monitoring if plan_state is provided
        if plan_state and hasattr(plan_state, 'current_step') and hasattr(plan_state, 'total_steps'):
            current_step = plan_state.current_step
            total_steps = plan_state.total_steps
            progress = f"{current_step}/{total_steps}"
                    
            logger.debug(
                "Plan execution monitoring enabled",
                extra={
                    "current_step": current_step,
                    "total_steps": total_steps,
                    "progress": progress,
                }
            )
                    
            # ğŸ”¥ ENHANCED: Add detailed plan steps to guide LLM
            plan_details = []
            if hasattr(plan_state, 'structured_plan') and plan_state.structured_plan:
                structured_plan = plan_state.structured_plan
                        
                # Build detailed step-by-step guidance
                plan_details.append(f"\n\nã€ğŸ“‹ è®¡åˆ’æ‰§è¡Œæ­¥éª¤ã€‘")
                plan_details.append(f"å½“å‰è¿›åº¦ï¼š{progress}\n")
                        
                for i, step in enumerate(structured_plan.steps, 1):
                    status_icon = "âœ…" if i < current_step else ("ğŸ”´" if i == current_step else "âšª")
                    step_status = "å·²å®Œæˆ" if i < current_step else ("è¿›è¡Œä¸­" if i == current_step else "å¾…æ‰§è¡Œ")
                            
                    plan_details.append(
                        f"{status_icon} **Step {i}: {step.name}**\n"
                        f"   - å·¥å…·ï¼š`{step.tool}`\n"
                        f"   - æè¿°ï¼š{step.description}\n"
                        f"   - çŠ¶æ€ï¼š{step_status}"
                    )
                            
                    # Add skill_command if available
                    if hasattr(step, 'skill_command') and step.skill_command:
                        plan_details.append(f"   - æŠ€èƒ½å‘½ä»¤ï¼š`{step.skill_command}`")
                            
                    plan_details.append("")
                        
                # Add specific guidance for current step
                if current_step <= len(structured_plan.steps):
                    current_step_obj = structured_plan.steps[current_step - 1]
                    plan_details.append(f"\nã€ğŸ¯ å½“å‰ä»»åŠ¡ã€‘\n")
                    plan_details.append(f"è¯·ç«‹å³æ‰§è¡Œ **Step {current_step}**: {current_step_obj.name}\n")
                    plan_details.append(f"ä½¿ç”¨å·¥å…·ï¼š`{current_step_obj.tool}`\n")
                    if hasattr(current_step_obj, 'skill_command') and current_step_obj.skill_command:
                        plan_details.append(f"æ‰§è¡Œå‘½ä»¤ï¼š`{current_step_obj.skill_command}`\n")
                    
            # Combine general guidance with detailed steps
            plan_guidance = (
                f"\n\nã€è®¡åˆ’æ‰§è¡Œç›‘æ§ã€‘\n"
                f"è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è®¡åˆ’æ­¥éª¤æ‰§è¡Œï¼Œç¡®ä¿æ¯ä¸€æ­¥ä½¿ç”¨æ­£ç¡®çš„å·¥å…·å’Œå‚æ•°ã€‚\n"
                f"å¦‚æœå½“å‰æ­¥éª¤æŒ‡å®šäº†å…·ä½“å‘½ä»¤ï¼ˆå¦‚ skill_commandï¼‰ï¼Œè¯·ç›´æ¥ä½¿ç”¨è¯¥å‘½ä»¤ï¼Œä¸è¦è‡ªå·±åˆ›é€ æ–°çš„å®ç°æ–¹å¼ã€‚\n"
                f"ç‰¹åˆ«æ˜¯ PDF/PPTXç”Ÿæˆä»»åŠ¡ï¼Œå¿…é¡»ä½¿ç”¨æä¾›çš„æŠ€èƒ½è„šæœ¬ï¼Œä¸è¦ä½¿ç”¨ inline Python ä»£ç ï¼\n"
            )
                    
            if plan_details:
                plan_guidance += "\n".join(plan_details)
                    
            # Inject into working messages
            working_messages.append({
                "role": "system",
                "content": plan_guidance,
            })
                
        # Get OpenAI tool definitions
        openai_tools = [tool.to_openai_tool() for tool in tools] if tools else None
        
        # Track iteration statistics
        actual_iterations = 0
        tool_calls_count = 0
        completed_early = False
        retry_without_tool_count = 0  # NEW: Track retry attempts when no tools called
        
        # Initialize strategy state for reflection and adjustment
        strategy_state = StrategyState()
        
        logger.debug(  # Changed from info: routine event
            "ReAct loop started",
            extra={
                "tools_count": len(tools) if tools else 0,
                "max_iterations": self.max_iterations,
                "session_id": session_id,
                "enable_reflection": self.enable_reflection,
            }
        )
        
        for iteration in range(self.max_iterations):
            actual_iterations = iteration + 1
            logger.debug(
                f"ReAct iteration {iteration + 1}/{self.max_iterations}"
            )
            
            try:
                # Call LLM with tools for function calling
                response = await self.llm_router.chat(
                    working_messages,
                    stream=False,
                    session_id=session_id,
                    tools=openai_tools,  # Pass tools for OpenAI function calling
                )
                
                # Check for tool calls in response
                tool_calls = self._extract_tool_calls(response)
                
                if tool_calls:
                    # Emit thinking event
                    yield {
                        "type": REACT_EVENT_THINKING,
                        "content": f"Iteration {iteration + 1}: Deciding to use tools",
                        "tool_calls": [tc.name for tc in tool_calls],
                    }
                    
                    # Process each tool call
                    for tool_call in tool_calls:
                        tool_calls_count += 1
                        
                        # Emit tool_call event
                        tool_call_event = {
                            "type": REACT_EVENT_TOOL_CALL,
                            "tool_call_id": tool_call.id,
                            "name": tool_call.name,
                            "arguments": tool_call.arguments,
                        }
                        logger.debug(  # Changed from info: routine tool_call event
                            "Emitting tool_call from react_loop",
                            extra={
                                "tool_call_id": tool_call.id,
                                "tool_call_name": tool_call.name,
                                "event_keys": list(tool_call_event.keys()),
                                "event_tool_call_id": tool_call_event.get("tool_call_id"),
                            }
                        )
                        
                        yield tool_call_event
                        
                        # Scenario 3: Pre-Action Reflection for high-risk operations
                        is_risky, risk_desc = self._is_high_risk_action(tool_call.name, tool_call.arguments)
                        if is_risky and self.enable_reflection:
                            risk_warning = (
                                f"ğŸš¨ **é«˜é£é™©æ“ä½œæ£€æµ‹**\n\n"
                                f"å³å°†æ‰§è¡Œï¼š{risk_desc}\n"
                                f"å·¥å…·ï¼š{tool_call.name}\n"
                                f"å‚æ•°ï¼š{str(tool_call.arguments)[:200]}\n\n"
                                f"è¯·å†æ¬¡ç¡®è®¤æ­¤æ“ä½œçš„å¿…è¦æ€§å’Œå®‰å…¨æ€§ã€‚"
                            )
                            
                            # Add pre-action reflection message
                            working_messages.append({
                                "role": "system",
                                "content": risk_warning,
                            })
                            
                            # Emit reflection event
                            yield {
                                "type": REACT_EVENT_REFLECTION,
                                "reflection_type": ReflectionType.PRE_ACTION.value,
                                "content": f"é«˜é£é™©æ“ä½œï¼š{risk_desc}",
                                "tool_name": tool_call.name,
                                "suggestion": "è¯·ç¡®è®¤æ“ä½œå¿…è¦æ€§ï¼Œè€ƒè™‘æ˜¯å¦æœ‰æ›´å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆ",
                                "risk_level": "high",
                            }
                            
                            logger.warning(
                                "High-risk action detected, reflection triggered",
                                extra={
                                    "tool_name": tool_call.name,
                                    "risk_description": risk_desc,
                                    "arguments": tool_call.arguments,
                                }
                            )
                        
                        # ğŸ”¥ğŸ”¥ğŸ”¥ CRITICAL: Inject SKILL.md guidance before execution (progressive disclosure)
                        skill_guidance = self._extract_skill_guidance_for_execution(tool_call.name, tool_call.arguments)
                        if skill_guidance:
                            logger.info(
                                "Injecting SKILL.md guidance before tool execution",
                                extra={
                                    "tool": tool_call.name,
                                    "skill_guidance_chars": len(skill_guidance),
                                    "arguments_preview": str(tool_call.arguments)[:100],
                                }
                            )
                            
                            # Add skill guidance message
                            working_messages.append({
                                "role": "system",
                                "content": skill_guidance,
                            })
                            
                            # ğŸ”¥ NEW: Emit event for user visibility (optional, for debugging)
                            yield {
                                "type": "skill_guidance_injected",
                                "tool_name": tool_call.name,
                                "guidance_length": len(skill_guidance),
                                "preview": skill_guidance[:200] + "..." if len(skill_guidance) > 200 else skill_guidance,
                            }
                        
                        # Execute tool - constraint checking is done in ToolManager
                        # to avoid duplication and centralize validation logic
                        try:
                            result = await self.tool_manager.execute(
                                tool_call.name,
                                tool_call.arguments,
                                skill_context=skill_context,  # Phase 2 - Pass skill context for tool restrictions
                            )
                        except Exception as e:
                            # Handle ToolNotAllowedError from ToolManager
                            from ..tools.manager import ToolNotAllowedError
                            if isinstance(e, ToolNotAllowedError):
                                logger.warning(
                                    "Tool call blocked by skill constraints",
                                    extra={
                                        "tool_name": tool_call.name,
                                        "allowed_tools": e.allowed_tools,
                                        "skill_context": getattr(skill_context, 'name', 'unknown') if skill_context else None,
                                    }
                                )
                                
                                # Add system message to inform LLM about the constraint
                                working_messages.append({
                                    "role": "system",
                                    "content": f"âš ï¸ å·¥å…· '{tool_call.name}' ä¸å¯ç”¨ã€‚{str(e)}"
                                })
                                
                                # Skip this tool call - continue to next iteration
                                continue
                            else:
                                # Re-raise other exceptions
                                raise
                        
                        # Check if this requires user confirmation - stop the loop
                        # Fix: Ensure metadata is a dict before accessing
                        metadata_dict = result.metadata if isinstance(result.metadata, dict) else {}
                        requires_confirmation = metadata_dict.get("requires_confirmation", False)
                        is_blocked = metadata_dict.get("is_blocked", False)
                        
                        # Emit tool_result event
                        logger.debug(  # Changed from info: routine tool_result event
                            "Emitting tool_result from react_loop",
                            extra={
                                "tool_call_id": tool_call.id,
                                "tool_call_name": tool_call.name,
                                "result_success": result.success,
                                "requires_confirmation": requires_confirmation,
                            }
                        )
                        
                        # Scenario 1: Check if result anomaly requires reflection (MUST reflect)
                        should_reflect, reflection_type, reason = self._should_reflect_on_result(result)
                        if should_reflect and self.enable_reflection:
                            # âœ… OPTIMIZE: Retrieve relevant memories from past experiences
                            memory_guidance = ""
                            if ERROR_LEARNING_AVAILABLE:
                                try:
                                    error_learning_service = get_error_learning_service(self.llm_router)
                                    
                                    # âœ… OPTIMIZE: Add timeout control to prevent blocking
                                    import asyncio
                                    retrieved_memories = await asyncio.wait_for(
                                        error_learning_service.retrieve_relevant_memories_for_error(
                                            error_type=reflection_type.value,
                                            error_message=(result.error or str(result.output))[:500],
                                            tool_name=tool_call.name,
                                        ),
                                        timeout=3.0,  # 3 seconds timeout
                                    )
                                    
                                    # Create memory injection prompt
                                    if retrieved_memories:
                                        memory_guidance = error_learning_service.create_memory_injection_prompt(
                                            retrieved_memories=retrieved_memories,
                                            current_error_type=reflection_type.value,
                                            current_error_message=(result.error or str(result.output))[:300],
                                        )
                                        
                                        logger.info(
                                            "Retrieved relevant memories for error correction",
                                            extra={
                                                "error_type": reflection_type.value,
                                                "memories_count": len(retrieved_memories),
                                                "top_score": retrieved_memories[0]["score"] if retrieved_memories else 0,
                                            }
                                        )
                                except asyncio.TimeoutError:
                                    logger.warning(
                                        "Memory retrieval timed out, proceeding without memory guidance",
                                        extra={"timeout_seconds": 3.0}
                                    )
                                    memory_guidance = ""
                                except Exception as e:
                                    logger.warning(
                                        "Failed to retrieve memories for error",
                                        extra={"error": str(e)}
                                    )
                            
                            # ğŸ”¥ NEW: Add skill path guidance for run_in_terminal errors
                            skill_path_guidance = ""
                            if (tool_call.name == "run_in_terminal" and 
                                skill_context and 
                                hasattr(skill_context, 'scripts_dir') and 
                                skill_context.scripts_dir):
                                skill_path_guidance = (
                                    f"\n\nğŸ’¡ **æŠ€èƒ½è„šæœ¬è·¯å¾„æç¤º**\n"
                                    f"- æŠ€èƒ½ `{skill_context.name}` çš„è„šæœ¬ç›®å½•ï¼š`{skill_context.scripts_dir}`\n"
                                    f"- âœ… æ­£ç¡®ç”¨æ³•ï¼š`python create_pdf_from_md.py ...`ï¼ˆç›´æ¥ä½¿ç”¨è„šæœ¬åï¼‰\n"
                                    f"- âŒ é”™è¯¯ç”¨æ³•ï¼š`python /workspace/.../create_pdf_from_md.py`ï¼ˆä¸è¦ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰\n"
                                )
                            
                            # âœ… OPTIMIZE: Add targeted error detection guidance
                            error_specific_guidance = ""
                            error_lower = (result.error or result.output or "").lower()
                            
                            if "module not found" in error_lower or "no such file" in error_lower:
                                error_specific_guidance = (
                                    f"\n\nğŸš¨ **å…³é”®é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨!**\n"
                                    f"æ£€æµ‹åˆ° `MODULE_NOT_FOUND` æˆ– `No such file` é”™è¯¯ã€‚\n\n"
                                    f"**è¯·ç«‹å³æ£€æŸ¥**:\n"
                                    f"1. âœ… è„šæœ¬æ–‡ä»¶æ˜¯å¦åœ¨æ­£ç¡®çš„ä½ç½®\n"
                                    f"2. âœ… æ˜¯å¦éœ€è¦å…ˆåˆ›å»ºè¯¥è„šæœ¬æ–‡ä»¶\n"
                                    f"3. âœ… è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼ˆå»ºè®®ä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–ç›´æ¥å†™è„šæœ¬åï¼‰\n"
                                    f"4. âœ… å·¥ä½œç›®å½•æ˜¯å¦æ­£ç¡®è®¾ç½®\n\n"
                                    f"**ç¤ºä¾‹ä¿®å¤**:\n"
                                    f"- âŒ é”™è¯¯ï¼š`node /wrong/path/create_presentation.js`\n"
                                    f"- âœ… æ­£ç¡®ï¼š`node skills/pptx/scripts/create_presentation.js ...`\n"
                                )
                            elif "permission denied" in error_lower:
                                error_specific_guidance = (
                                    f"\n\nğŸš¨ **æƒé™é”™è¯¯**!\n"
                                    f"æ£€æµ‹åˆ° `Permission denied` é”™è¯¯ã€‚\n\n"
                                    f"**è§£å†³æ–¹æ¡ˆ**:\n"
                                    f"1. âœ… æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ‰§è¡Œæƒé™ï¼š`chmod +x script.py`\n"
                                    f"2. âœ… æ£€æŸ¥ç›®å½•æ˜¯å¦æœ‰å†™å…¥æƒé™\n"
                                    f"3. âœ… å°è¯•ä½¿ç”¨ `python script.py` è€Œä¸æ˜¯ç›´æ¥æ‰§è¡Œ\n"
                                )
                            elif "command not found" in error_lower or "not recognized" in error_lower:
                                error_specific_guidance = (
                                    f"\n\nğŸš¨ **å‘½ä»¤ä¸å­˜åœ¨**!\n"
                                    f"æ£€æµ‹åˆ° `Command not found` é”™è¯¯ã€‚\n\n"
                                    f"**è§£å†³æ–¹æ¡ˆ**:\n"
                                    f"1. âœ… æ£€æŸ¥å‘½ä»¤æ˜¯å¦å·²å®‰è£…\n"
                                    f"2. âœ… æ£€æŸ¥ PATH ç¯å¢ƒå˜é‡\n"
                                    f"3. âœ… ä½¿ç”¨å®Œæ•´è·¯å¾„æˆ–ç¡®è®¤å‘½ä»¤åç§°æ­£ç¡®\n"
                                )
                            
                            reflection_content = (
                                f"âš ï¸ **ç»“æœå¼‚å¸¸æ£€æµ‹**\n\n"
                                f"{reason}\n\n"
                                f"å·¥å…·ï¼š{tool_call.name}\n"
                                f"ç»“æœï¼š{result.output[:100] if result.output else 'N/A'}...\n"
                                f"{skill_path_guidance}"
                                f"{error_specific_guidance}"
                                f"{memory_guidance}"  # âœ… OPTIMIZE: Add memory guidance
                            )
                            
                            # Add reflection message
                            working_messages.append({
                                "role": "system",
                                "content": reflection_content,
                            })
                            
                            # Emit reflection event
                            yield {
                                "type": REACT_EVENT_REFLECTION,
                                "reflection_type": reflection_type.value,
                                "content": reason,
                                "tool_name": tool_call.name,
                                "suggestion": "è¯·æ£€æŸ¥å·¥å…·å‚æ•°å’Œæ‰§è¡Œç¯å¢ƒï¼Œæˆ–å°è¯•ä½¿ç”¨å…¶ä»–æ–¹æ³•",
                            }
                            
                            logger.info(
                                "Result anomaly reflection triggered",
                                extra={
                                    "tool_name": tool_call.name,
                                    "reflection_type": reflection_type.value,
                                    "reason": reason,
                                }
                            )
                        
                        # P4-4 NEW: Multi-dimension reflection checks (only if no anomaly reflection)
                        elif self.enable_reflection and plan_state:
                            # Check 2: Step stuck reflection
                            is_stuck, stuck_reason = self._should_step_stuck_reflect(iteration, plan_state, strategy_state)
                            if is_stuck:
                                # ğŸ”¥ NEW: Add skill path guidance if skill_context is available
                                skill_path_guidance = ""
                                if skill_context and hasattr(skill_context, 'scripts_dir') and skill_context.scripts_dir:
                                    skill_path_guidance = (
                                        f"\n\nğŸ’¡ **æŠ€èƒ½è„šæœ¬è·¯å¾„æç¤º**\n"
                                        f"- æŠ€èƒ½ `{skill_context.name}` çš„è„šæœ¬ç›®å½•ï¼š`{skill_context.scripts_dir}`\n"
                                        f"- âœ… æ­£ç¡®ç¤ºä¾‹ï¼š`python create_pdf_from_md.py ...`ï¼ˆç›´æ¥ä½¿ç”¨è„šæœ¬åï¼‰\n"
                                        f"- âŒ é”™è¯¯ç¤ºä¾‹ï¼š`python /workspace/.../create_pdf_from_md.py`ï¼ˆä¸è¦ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰\n"
                                    )
                                
                                # âœ… OPTIMIZE: Add repeated failure warning
                                repeated_failure_warning = ""
                                if len(strategy_state.reflection_history) >= 2:
                                    repeated_failure_warning = (
                                        f"\n\nğŸš¨ **è­¦å‘Šï¼šå·²åæ€ {len(strategy_state.reflection_history)} æ¬¡ä½†ä»æœªè¿›å±•**!\n"
                                        f"å½“å‰æ–¹æ³•å¯èƒ½æ ¹æœ¬ä¸å¯è¡Œï¼Œè¯·ç«‹å³:\n"
                                        f"1. â›” **åœæ­¢å½“å‰å°è¯•**\n"
                                        f"2. ğŸ’¡ **å½»åº•æ¢ä¸€ç§æ€è·¯**\n"
                                        f"3. ğŸ†˜ **æˆ–è¯·æ±‚ç”¨æˆ·å¸®åŠ©**\n"
                                    )
                                                        
                                reflection_content = (
                                    f"âš ï¸ **Step åœæ»æ£€æµ‹**\n\n"
                                    f"{stuck_reason}\n\n"
                                    f"å»ºè®®æ“ä½œ:\n"
                                    f"1. å¦‚æœæ­£åœ¨é‡å¤åŒä¸€å·¥å…· â†’ ç«‹å³åœæ­¢ï¼Œå°è¯•å…¶ä»–æ–¹æ³•\n"
                                    f"2. å¦‚æœ Step åœæ» â†’ è¯„ä¼°æ˜¯å¦å¯ä»¥è¿›å…¥ä¸‹ä¸€æ­¥\n"
                                    f"3. å¦‚æœå·¥å…·åç¦» â†’ å›é¡¾è®¡åˆ’ä¸­çš„å·¥å…·çº¦æŸ\n"
                                    f"4. å¦‚æœä»»åŠ¡å›°éš¾ â†’ è€ƒè™‘åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡\n"
                                    f"{skill_path_guidance}"
                                    f"{repeated_failure_warning}"
                                )
                                
                                working_messages.append({
                                    "role": "system",
                                    "content": reflection_content,
                                })
                                
                                yield {
                                    "type": REACT_EVENT_REFLECTION,
                                    "reflection_type": ReflectionType.STRATEGY_ADJUSTMENT.value,
                                    "content": stuck_reason,
                                    "suggestion": "è¯·é‡æ–°è¯„ä¼°å½“å‰ç­–ç•¥ï¼Œé¿å…é™·å…¥å¾ªç¯",
                                }
                                
                                logger.warning(
                                    "Step stuck reflection triggered",
                                    extra={
                                        "iteration": iteration,
                                        "reason": stuck_reason,
                                        "current_step": getattr(plan_state, 'current_step', 'N/A'),
                                    }
                                )
                            
                            # Check 3: Slow progress reflection
                            elif self._should_long_task_reflect(iteration, self.max_iterations, plan_state):
                                reflection_content = (
                                    f"âš ï¸ **è¿›åº¦ç¼“æ…¢æ£€æµ‹**\n\n"
                                    f"å½“å‰è¿­ä»£ï¼š{iteration + 1}/{self.max_iterations}\n"
                                    f"å½“å‰æ­¥éª¤ï¼š{getattr(plan_state, 'current_step', 'N/A')}/{getattr(plan_state, 'total_steps', 'N/A')}\n"
                                    f"å·¥å…·æ‰§è¡Œæ¬¡æ•°ï¼š{getattr(plan_state, 'tool_execution_count', 'N/A')}\n\n"
                                    f"å»ºè®®æ“ä½œ:\n"
                                    f"1. ç®€åŒ–å½“å‰æ–¹æ³•ï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–\n"
                                    f"2. å¦‚æœå·²å®Œæˆè¶³å¤Ÿä¿¡æ¯æ”¶é›†ï¼Œè€ƒè™‘è¿›å…¥ä¸‹ä¸€æ­¥\n"
                                    f"3. å¦‚éœ€å¸®åŠ©ï¼Œè¯·å‘ç”¨æˆ·è¯·æ±‚æ›´æ˜ç¡®çš„æŒ‡å¯¼\n"
                                )
                                
                                working_messages.append({
                                    "role": "system",
                                    "content": reflection_content,
                                })
                                
                                yield {
                                    "type": REACT_EVENT_REFLECTION,
                                    "reflection_type": ReflectionType.LONG_TASK.value,
                                    "content": "è¿›åº¦ç¼“æ…¢æ£€æµ‹ï¼šè¿ç»­ 2 æ¬¡è¿­ä»£æ— è¿›å±•",
                                    "suggestion": "è¯·åŠ å¿«æ‰§è¡ŒèŠ‚å¥æˆ–è°ƒæ•´ç­–ç•¥",
                                }
                                
                                logger.warning(
                                    "Slow progress reflection triggered",
                                    extra={
                                        "iteration": iteration,
                                        "current_step": getattr(plan_state, 'current_step', 'N/A'),
                                        "tool_execution_count": getattr(plan_state, 'tool_execution_count', 'N/A'),
                                    }
                                )
                        
                        yield {
                            "type": REACT_EVENT_TOOL_RESULT,
                            "tool_call_id": tool_call.id,
                            "tool_name": tool_call.name,
                            "success": result.success,
                            "output": result.output[:500] if result.output else "",
                            "error": result.error,
                            "result": {
                                "success": result.success,
                                "output": result.output,
                                "error": result.error,
                                "requires_confirmation": requires_confirmation,
                                "is_blocked": is_blocked,
                                # Fix: Ensure metadata is a dict before accessing
                                "confirmation_id": result.metadata.get("confirmation_id", "") if isinstance(result.metadata, dict) else "",
                                "command": result.metadata.get("command", "") if isinstance(result.metadata, dict) else "",
                            },
                        }
                        
                        # If command requires confirmation or is blocked, stop the loop
                        # User must confirm before the command can be executed
                        if requires_confirmation:
                            logger.info(
                                "ReAct loop paused - awaiting user confirmation",
                                extra={
                                    "tool_call_id": tool_call.id,
                                    "confirmation_id": metadata_dict.get("confirmation_id"),
                                }
                            )
                            # Emit a waiting event to tell frontend to wait for user
                            yield {
                                "type": "awaiting_confirmation",
                                "tool_call_id": tool_call.id,
                                "confirmation_id": metadata_dict.get("confirmation_id"),
                                "command": metadata_dict.get("command"),
                            }
                            # Don't continue the loop - wait for user action
                            return
                        
                        # Record tool execution for pattern analysis
                        execution_record = ToolExecutionRecord(
                            tool_name=tool_call.name,
                            arguments=tool_call.arguments,
                            success=result.success,
                            output=result.output if result.output else "",
                            error=result.error,
                            iteration=actual_iterations,
                        )
                        strategy_state.tool_execution_history.append(execution_record)
                        
                        # Update strategy state
                        self._update_strategy_state(strategy_state, tool_call.name, result.success)
                        
                        # Add tool result to messages
                        working_messages.append({
                            "role": "assistant",
                            "content": None,
                            "tool_calls": [{
                                "id": tool_call.id,
                                "type": "function",
                                "function": {
                                    "name": tool_call.name,
                                    "arguments": json.dumps(tool_call.arguments),
                                }
                            }]
                        })
                        working_messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": result.output if result.success else f"Error: {result.error}",
                        })
                        
                        # ===== REFLECTION: Analyze tool execution result =====
                        if self.enable_reflection:
                            reflection = await self._reflect_on_tool_result(
                                tool_call.name,
                                result,
                                strategy_state,
                            )
                            
                            if reflection.should_adjust and strategy_state.adjustment_count < self.MAX_ADJUSTMENTS:
                                strategy_state.adjustment_count += 1
                                strategy_state.reflection_history.append(ReflectionRecord(
                                    iteration=actual_iterations,
                                    reflection_type=ReflectionType.TOOL_RESULT.value,
                                    reason=reflection.reason,
                                    suggestion=reflection.suggestion,
                                ))
                                
                                # Emit reflection event
                                yield {
                                    "type": REACT_EVENT_REFLECTION,
                                    "reflection_type": ReflectionType.TOOL_RESULT.value,
                                    "content": reflection.reason,
                                    "suggestion": reflection.suggestion,
                                    "confidence": reflection.confidence,
                                }
                                
                                # Emit strategy adjustment event
                                yield {
                                    "type": REACT_EVENT_STRATEGY_ADJUSTMENT,
                                    "reason": reflection.reason,
                                    "suggestion": reflection.suggestion,
                                    "adjustment_count": strategy_state.adjustment_count,
                                }
                                
                                # Add reflection guidance to messages
                                working_messages.append({
                                    "role": "system",
                                    "content": f"ğŸ¤” **æ‰§è¡Œåæ€**\n\n{reflection.reason}\n\n**å»ºè®®è°ƒæ•´**ï¼š{reflection.suggestion}",
                                })
                    
                    # Continue to next iteration
                    continue
                
                # No tool calls - we have the final answer
                final_content = response.content if hasattr(response, 'content') else str(response)
                
                # ===== SCHEME 1: Check if tool calls were required but not made =====
                # Detect if user message requires tool calls but LLM didn't call any
                if self._requires_tool_call_but_none_made(messages):
                    retry_without_tool_count += 1
                    
                    # Enforce iteration < 2 retryè§„èŒƒ: Only retry in first 2 iterations
                    if retry_without_tool_count >= self.MAX_RETRY_WITHOUT_TOOL or iteration >= 2:
                        # Exceeded retry limit or past early iterations - fail with error
                        logger.error(
                            "LLM persistently not calling tools, task cannot continue",
                            extra={
                                "iteration": iteration + 1,
                                "retry_count": retry_without_tool_count,
                                "max_retry": self.MAX_RETRY_WITHOUT_TOOL,
                            }
                        )
                        yield {
                            "type": REACT_EVENT_ERROR,
                            "error": f"LLMæŒç»­ä¸è°ƒç”¨å·¥å…·ï¼ˆé‡è¯•{retry_without_tool_count}æ¬¡ï¼‰ï¼Œä»»åŠ¡æ— æ³•ç»§ç»­",
                            "retry_count": retry_without_tool_count,
                        }
                        return  # Terminate loop
                    
                    logger.warning(
                        "LLM responded without tool calls when tools were required (retrying)",
                        extra={
                            "iteration": iteration + 1,
                            "retry_count": retry_without_tool_count,
                            "response_preview": final_content[:200],
                        }
                    )
                    
                    # If this is within the first 2 iterations and no tools were called at all,
                    # give LLM another chance with explicit reminder
                    # Changed from (iteration == 0) to (iteration < 2) to allow more attempts
                    if iteration < 2 and tool_calls_count == 0:
                        # Add a system reminder to use tools
                        working_messages.append({
                            "role": "system",
                            "content": "âš ï¸ æ³¨æ„ï¼šä½ éœ€è¦è°ƒç”¨å®é™…çš„å·¥å…·æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼Œè€Œä¸æ˜¯åªç”¨æ–‡å­—å›å¤ã€‚\n\n"
                                      "å½“ç”¨æˆ·è¦æ±‚åˆ›å»º/ç”Ÿæˆ/åˆ¶ä½œä»»ä½•å…·ä½“äº§ç‰©ï¼ˆå¦‚ PPTã€æ–‡ä»¶ã€ä»£ç ç­‰ï¼‰æ—¶ï¼š\n"
                                      "1. å¿…é¡»ç«‹å³è°ƒç”¨ç›¸åº”çš„å·¥å…·ï¼ˆread_file, write_file, run_in_terminalï¼‰\n"
                                      "2. ç»ä¸èƒ½ç”¨æ–‡å­—å£°ç§°'å·²ç»å®Œæˆ'è€Œä¸å®é™…è°ƒç”¨å·¥å…·\n"
                                      "3. åªæœ‰åœ¨å·¥å…·çœŸæ­£æ‰§è¡ŒæˆåŠŸåæ‰èƒ½å‘ŠçŸ¥ç”¨æˆ·å®Œæˆ\n\n"
                                      "è¯·é‡æ–°æ€è€ƒå¹¶è°ƒç”¨é€‚å½“çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚"
                        })
                        # Continue to next iteration to let LLM try again
                        continue
                
                # ===== NEW: Check if LLM repeatedly violated tool constraints =====
                # If LLM keeps trying forbidden tools, provide explicit guidance
                if tool_calls_count == 0 and iteration >= 1:
                    # No valid tool calls made in this iteration
                    # Check if there were constraint violations
                    working_messages.append({
                        "role": "system",
                        "content": "ğŸ’¡ æç¤ºï¼šä½ åˆšæ‰å°è¯•ä½¿ç”¨çš„å·¥å…·ä¸åœ¨å½“å‰æŠ€èƒ½çš„å…è®¸åˆ—è¡¨ä¸­ã€‚\n\n"
                                  "è¯·ä»”ç»†æŸ¥çœ‹ System Prompt ä¸­çš„æŠ€èƒ½è¯´æ˜ï¼Œåªä½¿ç”¨æ˜ç¡®åˆ—å‡ºçš„å·¥å…·ã€‚\n"
                                  "å¦‚æœä¸ç¡®å®šåº”è¯¥ç”¨ä»€ä¹ˆå·¥å…·ï¼Œè¯·å…ˆåˆ†æä»»åŠ¡éœ€æ±‚ï¼Œç„¶åé€‰æ‹©æœ€åŒ¹é…çš„å·¥å…·ã€‚"
                    })
                
                completed_early = True
                
                # ===== REFLECTION: Verify final answer before returning =====
                if self.enable_reflection:
                    # Get original user query
                    original_query = ""
                    for msg in messages:
                        if msg.get("role") == "user":
                            original_query = msg.get("content", "")
                            break
                    
                    final_reflection = await self._reflect_on_final_answer(
                        final_content,
                        original_query,
                        working_messages,
                    )
                    
                    if final_reflection.should_adjust:
                        # Add verification feedback and give LLM one more chance
                        working_messages.append({
                            "role": "system",
                            "content": f"ğŸ” **æœ€ç»ˆç­”æ¡ˆéªŒè¯**\n\n{final_reflection.reason}\n\n**æ”¹è¿›å»ºè®®**ï¼š{final_reflection.suggestion}",
                        })
                        
                        yield {
                            "type": REACT_EVENT_REFLECTION,
                            "reflection_type": ReflectionType.FINAL_VERIFICATION.value,
                            "content": final_reflection.reason,
                            "suggestion": final_reflection.suggestion,
                        }
                        
                        # Continue to next iteration for improvement
                        continue
                
                logger.debug(  # Changed from info: routine completion event
                    "ReAct loop completed",
                    extra={
                        "iterations": iteration + 1,
                        "response_length": len(final_content),
                        "strategy_summary": self.get_strategy_summary(strategy_state),
                    }
                )
                
                yield {
                    "type": REACT_EVENT_FINAL,
                    "content": final_content,
                    "reflection_count": len(strategy_state.reflection_history),
                    "adjustment_count": strategy_state.adjustment_count,
                }
                return
                
            except Exception as e:
                logger.error(
                    "ReAct iteration failed",
                    extra={
                        "iteration": iteration + 1,
                        "error": str(e),
                        "error_type": type(e).__name__,
                    }
                )
                
                # ALWAYS trigger failure reflection on exception (regardless of enable_reflection)
                failure_analysis = self._generate_failure_analysis(strategy_state, iteration + 1)
                
                # Emit reflection event
                yield {
                    "type": REACT_EVENT_REFLECTION,
                    "reflection_type": ReflectionType.FAILURE_ANALYSIS.value,
                    "content": f"å¼‚å¸¸é€€å‡º: {str(e)}",
                    "suggestion": failure_analysis["recommendation"],
                    "failure_details": failure_analysis,
                    "exception_type": type(e).__name__,
                }
                
                # Emit error event
                yield {
                    "type": REACT_EVENT_ERROR,
                    "error": str(e),
                    "iteration": iteration + 1,
                    "failure_analysis": failure_analysis,
                }
                
                # Terminate loop on exception
                logger.warning(
                    "ReAct loop terminated by exception",
                    extra={
                        "iteration": iteration + 1,
                        "exception": str(e),
                        "failure_analysis": failure_analysis,
                    }
                )
                
                # âœ… OPTIMIZE: Record error for learning if this was a tool execution error
                if ERROR_LEARNING_AVAILABLE:
                    try:
                        error_learning_service = get_error_learning_service(self.llm_router)
                        
                        # Record the error pattern
                        error_learning_service.record_error(
                            error_type=type(e).__name__,
                            error_message=str(e),
                            tool_name=tool_call.name if 'tool_call' in locals() else "unknown",
                            session_id=session_id or "unknown",
                            context={"iteration": iteration},
                        )
                    except Exception as learn_error:
                        logger.debug(
                            "Failed to record error for learning",
                            extra={"error": str(learn_error)}
                        )
                
                return
        
        # Max iterations reached
        utilization_rate = (actual_iterations / self.max_iterations * 100) if self.max_iterations > 0 else 0
        
        # Generate failure analysis
        failure_analysis = self._generate_failure_analysis(strategy_state, actual_iterations)
        
        logger.warning(
            "ReAct loop reached max iterations",
            extra={
                "actual_iterations": actual_iterations,
                "max_iterations": self.max_iterations,
                "utilization_rate": f"{utilization_rate:.1f}%",
                "total_tool_calls": tool_calls_count,
                "completed_early": completed_early,
                "session_id": session_id,
                "strategy_summary": self.get_strategy_summary(strategy_state),
                "failure_analysis": failure_analysis,
            }
        )
        
        # ALWAYS emit failure reflection event when max iterations reached
        # This helps with debugging and understanding why the loop didn't complete
        yield {
            "type": REACT_EVENT_REFLECTION,
            "reflection_type": ReflectionType.FAILURE_ANALYSIS.value,
            "content": failure_analysis["primary_reason"],
            "suggestion": failure_analysis["recommendation"],
            "failure_details": failure_analysis,
        }
        
        yield {
            "type": REACT_EVENT_ERROR,
            "error": f"Maximum iterations ({self.max_iterations}) reached without completing the task",
            "failure_analysis": failure_analysis,
            "strategy_summary": self.get_strategy_summary(strategy_state),
        }
    
    def _extract_tool_calls(self, response: Any) -> list[ToolCallRequest]:
        """Extract tool calls from LLM response.
        
        Handles different response formats from different providers:
        1. OpenAI standard function calling format
        2. XML format (used by some models like qwen)
        
        Args:
            response: LLM response object
            
        Returns:
            List of ToolCallRequest objects
        """
        tool_calls = []
        
        # Try LLMResponse with tool_calls field (our format)
        if hasattr(response, 'tool_calls') and response.tool_calls:
            for tc in response.tool_calls:
                # Handle dict format (from BailianProvider)
                if isinstance(tc, dict):
                    func = tc.get('function', {})
                    args = func.get('arguments', '{}')
                    if isinstance(args, str):
                        try:
                            arguments = json.loads(args)
                        except json.JSONDecodeError:
                            arguments = {"raw": args}
                    else:
                        arguments = args
                    
                    tool_calls.append(ToolCallRequest(
                        id=tc.get('id', ''),
                        name=func.get('name', ''),
                        arguments=arguments,
                    ))
                # Handle OpenAI object format
                elif hasattr(tc, 'function'):
                    args = tc.function.arguments
                    if isinstance(args, str):
                        try:
                            arguments = json.loads(args)
                        except json.JSONDecodeError:
                            arguments = {"raw": args}
                    else:
                        arguments = args
                    
                    tool_calls.append(ToolCallRequest(
                        id=tc.id,
                        name=tc.function.name,
                        arguments=arguments,
                    ))
        
        # Try dict format (legacy)
        elif isinstance(response, dict):
            if 'tool_calls' in response:
                for tc in response['tool_calls']:
                    args = tc.get('function', {}).get('arguments', {})
                    if isinstance(args, str):
                        try:
                            args = json.loads(args)
                        except:
                            args = {"raw": args}
                    
                    tool_calls.append(ToolCallRequest(
                        id=tc.get('id', ''),
                        name=tc.get('function', {}).get('name', ''),
                        arguments=args,
                    ))
        
        # Try XML format tool calls (for models that don't support standard function calling)
        # Format: <function=name>\n<parameter=param_name>\nvalue\n</parameter>\n</function>
        if not tool_calls and hasattr(response, 'content') and response.content:
            xml_tool_calls = self._parse_xml_tool_calls(response.content)
            tool_calls.extend(xml_tool_calls)
        
        logger.debug(
            "Tool calls extracted",
            extra={"count": len(tool_calls), "names": [tc.name for tc in tool_calls]}
        )
        
        return tool_calls
    
    def _requires_tool_call_but_none_made(self, messages: list[dict]) -> bool:
        """Check if user message requires tool calls but LLM didn't make any.
        
        Detects common patterns that typically require tool execution:
        - File creation/modification requests
        - Code execution requests
        - Terminal command needs
        - Web search requests
        
        Args:
            messages: Conversation messages
            
        Returns:
            True if tool calls were likely required but not made
        """
        # Get last user message
        last_user_message = None
        for msg in reversed(messages):
            if msg.get('role') == 'user':
                last_user_message = msg.get('content', '')
                break
        
        if not last_user_message:
            return False
        
        # Keywords that typically require tool calls
        tool_required_patterns = [
            # File operations
            r'åˆ›å»º.*æ–‡ä»¶|create.*file|write.*file|save.*file',
            r'åˆ é™¤.*æ–‡ä»¶|delete.*file|remove.*file',
            r'ç§»åŠ¨.*æ–‡ä»¶|move.*file|rename.*file',
            r'è¯»å–.*æ–‡ä»¶|read.*file|open.*file',
            
            # PPT/Document creation - expanded patterns
            r'åˆ›å»º.*PPT|create.*PPT|make.*presentation|generate.*PPT',
            r'åˆ›å»º.*æ–‡æ¡£|create.*document|make.*doc',
            r'ç”Ÿæˆ.*PPT|generate.*presentation',
            r'åˆ¶ä½œ.*å¹»ç¯ç‰‡|make.*slides',
            r'åˆ¶ä½œ.*PPT|åˆ¶ä½œä¸€ä¸ª.*PPT',  # "åˆ¶ä½œPPT" or "åˆ¶ä½œä¸€ä¸ªPPT"
            r'åš.*PPT|åšä¸€ä¸ª.*PPT',  # "åšPPT" or "åšä¸€ä¸ªPPT"
            r'éœ€è¦.*PPT|éœ€è¦åˆ¶ä½œ.*PPT',  # "éœ€è¦PPT" or "éœ€è¦åˆ¶ä½œPPT"
            r'å†™.*è„šæœ¬|write.*script|create.*script',
            
            # Code execution
            r'è¿è¡Œ.*ä»£ç |run.*code|execute.*script',
            r'æ‰§è¡Œ.*å‘½ä»¤|execute.*command|run.*command',
            
            # Terminal operations
            r'å®‰è£….*åº“|install.*package|pip install',
            r'åˆ›å»ºç›®å½•|create.*directory|mkdir',
            
            # Web search
            r'æœç´¢.*ä¿¡æ¯|search.*information|look up',
            
            # Skill commands (CRITICAL: These MUST trigger tool calls)
            r'/pptx\s+',  # /pptx command followed by whitespace
            r'/xlsx\s+',  # /xlsx command
            r'/pdf\s+',   # /pdf command
            r'/skill\s+', # /skill command
        ]
        
        import re
        message_lower = last_user_message.lower()
        
        for pattern in tool_required_patterns:
            if re.search(pattern, message_lower, re.IGNORECASE):
                return True
        
        return False
    
    def _parse_xml_tool_calls(self, content: str) -> list[ToolCallRequest]:
        """Parse XML format tool calls from response content.
        
        Handles format like:
        <function=list_dir>
        <parameter=path>
        ~/Documents/x-agent/
        </parameter>
        </function>
        
        Also handles multiple parameters:
        <function=write_file>
        <parameter=path>
        /path/to/file
        </parameter>
        <parameter=content>
        file content here
        </parameter>
        </function>
        
        Args:
            content: Response text content
            
        Returns:
            List of ToolCallRequest objects
        """
        tool_calls = []
        
        # Match <function=name>...</function> blocks
        function_pattern = r'<function=([^>]+)>(.*?)</function>'
        function_matches = re.findall(function_pattern, content, re.DOTALL)
        
        for func_name, func_body in function_matches:
            arguments = {}
            
            # Match <parameter=name>value</parameter> within function body
            param_pattern = r'<parameter=([^>]+)>(.*?)</parameter>'
            param_matches = re.findall(param_pattern, func_body, re.DOTALL)
            
            for param_name, param_value in param_matches:
                # Clean up parameter value (strip whitespace)
                arguments[param_name.strip()] = param_value.strip()
            
            # If no parameters found but function has content, use as single argument
            if not arguments and func_body.strip():
                arguments["value"] = func_body.strip()
            
            tool_calls.append(ToolCallRequest(
                id=f"xml_{func_name}_{len(tool_calls)}",
                name=func_name.strip(),
                arguments=arguments,
            ))
            
            logger.info(
                "Parsed XML tool call",
                extra={"function": func_name, "arguments": arguments}
            )
        
        return tool_calls
    
    def _update_strategy_state(
        self,
        state: StrategyState,
        tool_name: str,
        success: bool,
    ) -> None:
        """Update strategy state based on tool execution result.
        
        Args:
            state: Current strategy state
            tool_name: Name of the executed tool
            success: Whether execution succeeded
        """
        if success:
            # Reset consecutive failures on success
            state.consecutive_failures = 0
        else:
            # Increment consecutive failures
            state.consecutive_failures += 1
            state.failed_tool_patterns.add(tool_name)
        
        # Track repeated tool usage
        if tool_name == state.last_tool_name:
            state.same_tool_repeated += 1
        else:
            state.same_tool_repeated = 0
            state.last_tool_name = tool_name
    
    async def _reflect_on_tool_result(
        self,
        tool_name: str,
        result: ToolResult,
        state: StrategyState,
    ) -> ReflectionResult:
        """Reflect on tool execution result and determine if strategy adjustment is needed.
        
        Args:
            tool_name: Name of the tool that was executed
            result: Tool execution result
            state: Current strategy state
            
        Returns:
            ReflectionResult with analysis and adjustment suggestion
        """
        # Case 1: Tool execution failed
        if not result.success:
            # Check for repeated failures
            if state.consecutive_failures >= self.MAX_CONSECUTIVE_FAILURES:
                return ReflectionResult(
                    should_adjust=True,
                    reason=f"å·¥å…· '{tool_name}' å·²è¿ç»­å¤±è´¥ {state.consecutive_failures} æ¬¡",
                    suggestion="è¯·å°è¯•ï¼š1) æ£€æŸ¥å‚æ•°æ˜¯å¦æ­£ç¡®ï¼›2) æ¢ç”¨å…¶ä»–å·¥å…·ï¼›3) è°ƒæ•´ä»»åŠ¡ç­–ç•¥",
                    confidence=0.8,
                )
            
            # Single failure - provide specific guidance based on error
            error_lower = (result.error or "").lower()
            if "not found" in error_lower or "ä¸å­˜åœ¨" in error_lower:
                return ReflectionResult(
                    should_adjust=True,
                    reason=f"ç›®æ ‡æ–‡ä»¶/è·¯å¾„ä¸å­˜åœ¨",
                    suggestion="è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å…ˆä½¿ç”¨ list_dir æŸ¥çœ‹å¯ç”¨æ–‡ä»¶",
                    confidence=0.7,
                )
            elif "permission" in error_lower or "æƒé™" in error_lower:
                return ReflectionResult(
                    should_adjust=True,
                    reason="æƒé™ä¸è¶³ï¼Œæ— æ³•æ‰§è¡Œè¯¥æ“ä½œ",
                    suggestion="è¯·å°è¯•å…¶ä»–æ–¹æ³•ï¼Œæˆ–å‘ç”¨æˆ·è¯´æ˜éœ€è¦æ›´é«˜æƒé™",
                    confidence=0.7,
                )
            elif "timeout" in error_lower or "è¶…æ—¶" in error_lower:
                return ReflectionResult(
                    should_adjust=True,
                    reason="æ“ä½œè¶…æ—¶",
                    suggestion="è¯·å°è¯•ç®€åŒ–æ“ä½œï¼Œæˆ–åˆ†æ­¥éª¤æ‰§è¡Œ",
                    confidence=0.6,
                )
            else:
                return ReflectionResult(
                    should_adjust=True,
                    reason=f"å·¥å…·æ‰§è¡Œå¤±è´¥: {result.error}",
                    suggestion="è¯·åˆ†æé”™è¯¯åŸå› ï¼Œè°ƒæ•´å‚æ•°åé‡è¯•ï¼Œæˆ–å°è¯•å…¶ä»–å·¥å…·",
                    confidence=0.5,
                )
        
        # Case 2: Tool succeeded but might need verification
        if result.success:
            # Check for empty or suspicious results
            # Note: Some tools returning empty results is normal (e.g., list_dir on empty directory)
            output = result.output or ""
            if len(output.strip()) == 0 and not self._is_empty_result_normal(tool_name):
                return ReflectionResult(
                    should_adjust=True,
                    reason=f"å·¥å…· '{tool_name}' è¿”å›äº†ç©ºç»“æœ",
                    suggestion="è¯·æ£€æŸ¥å‚æ•°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•å…¶ä»–å·¥å…·è·å–ä¿¡æ¯",
                    confidence=0.6,
                )
            
            # Check for repeated same tool usage (possible loop)
            if state.same_tool_repeated >= self.MAX_SAME_TOOL_REPEATS:
                return ReflectionResult(
                    should_adjust=True,
                    reason=f"è¿ç»­å¤šæ¬¡ä½¿ç”¨åŒä¸€å·¥å…· '{tool_name}'ï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯",
                    suggestion="è¯·é‡æ–°è¯„ä¼°ä»»åŠ¡ç­–ç•¥ï¼Œå°è¯•ä¸åŒçš„æ–¹æ³•æˆ–å·¥å…·ç»„åˆ",
                    confidence=0.75,
                )
        
        # No adjustment needed
        return ReflectionResult(
            should_adjust=False,
            reason="å·¥å…·æ‰§è¡ŒæˆåŠŸ",
            suggestion="ç»§ç»­æ‰§è¡Œ",
            confidence=0.9,
        )
    
    async def _reflect_on_plan_progress(
        self,
        original_plan: str,
        completed_steps: list[str],
        current_step: int,
        state: StrategyState,
    ) -> ReflectionResult:
        """Reflect on plan execution progress and suggest adjustments.
        
        Args:
            original_plan: Original plan text
            completed_steps: List of completed step descriptions
            current_step: Current step number
            state: Current strategy state
            
        Returns:
            ReflectionResult with plan adjustment suggestion
        """
        total_steps = len(original_plan.split("\n")) if original_plan else 0
        progress = len(completed_steps) / total_steps if total_steps > 0 else 0
        
        # Check if progress is too slow
        if state.adjustment_count < self.MAX_ADJUSTMENTS:
            if progress < 0.3 and state.consecutive_failures > 0:
                return ReflectionResult(
                    should_adjust=True,
                    reason="ä»»åŠ¡è¿›å±•ç¼“æ…¢ï¼Œé‡åˆ°å¤šæ¬¡å¤±è´¥",
                    suggestion="å»ºè®®ç®€åŒ–è®¡åˆ’ï¼Œä¼˜å…ˆå®Œæˆæ ¸å¿ƒä»»åŠ¡ï¼Œæˆ–å¯»æ±‚ç”¨æˆ·æ¾„æ¸…",
                    confidence=0.7,
                    adjusted_plan=None,  # Could generate simplified plan here
                )
            
            # Check if too many steps have been attempted
            if len(state.tool_execution_history) > total_steps * 2:
                return ReflectionResult(
                    should_adjust=True,
                    reason="æ‰§è¡Œæ­¥éª¤æ•°è¿œè¶…è®¡åˆ’æ­¥éª¤ï¼Œå¯èƒ½å­˜åœ¨æ•ˆç‡é—®é¢˜",
                    suggestion="è¯·é‡æ–°è¯„ä¼°å½“å‰æ–¹æ³•ï¼Œè€ƒè™‘æ›´ç›´æ¥çš„è§£å†³æ–¹æ¡ˆ",
                    confidence=0.6,
                )
        
        return ReflectionResult(
            should_adjust=False,
            reason="è®¡åˆ’æ‰§è¡Œæ­£å¸¸",
            suggestion="ç»§ç»­æŒ‰è®¡åˆ’æ‰§è¡Œ",
            confidence=0.8,
        )
    
    async def _reflect_on_final_answer(
        self,
        draft_answer: str,
        original_query: str,
        messages: list[dict[str, str]],
    ) -> ReflectionResult:
        """Reflect on final answer before returning to user.
        
        Args:
            draft_answer: Draft final answer
            original_query: Original user query
            messages: Full conversation history
            
        Returns:
            ReflectionResult with verification result
        """
        # Check for incomplete indicators - uncertainty expressions
        incomplete_indicators = [
            "æˆ‘ä¸ç¡®å®š", "å¯èƒ½", "å¤§æ¦‚", "ä¹Ÿè®¸", "ä¸ç¡®å®š", "ä¸æ¸…æ¥š",
            "i'm not sure", "maybe", "possibly", "perhaps", "uncertain",
        ]
        if any(indicator in draft_answer.lower() for indicator in incomplete_indicators):
            return ReflectionResult(
                should_adjust=True,
                reason="å›ç­”ä¸­åŒ…å«ä¸ç¡®å®šæ€§è¡¨è¿°",
                suggestion="å¦‚æœä¿¡æ¯ä¸ç¡®å®šï¼Œè¯·æ˜ç¡®è¯´æ˜ï¼Œæˆ–å°è¯•è·å–æ›´å¤šå¯é ä¿¡æ¯",
                confidence=0.6,
            )
        
        # Check for placeholder or incomplete response patterns
        incomplete_patterns = [
            r"æˆ‘éœ€è¦.*æ‰èƒ½", r"è¯·æä¾›.*ä¿¡æ¯", r"ç¼ºå°‘.*æ•°æ®",
            r"i need.*to", r"please provide.*information", r"missing.*data",
        ]
        import re
        for pattern in incomplete_patterns:
            if re.search(pattern, draft_answer.lower()):
                return ReflectionResult(
                    should_adjust=True,
                    reason="å›ç­”æš—ç¤ºéœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½å®Œæˆ",
                    suggestion="è¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·éœ€è¦å“ªäº›å…·ä½“ä¿¡æ¯ï¼Œæˆ–å°è¯•ç”¨ç°æœ‰ä¿¡æ¯å°½å¯èƒ½å›ç­”",
                    confidence=0.65,
                )
        
        # Check answer length appropriateness
        # Too short might be incomplete, too long might be unfocused
        answer_length = len(draft_answer)
        if answer_length < 20 and len(original_query) > 20:
            return ReflectionResult(
                should_adjust=True,
                reason="å›ç­”è¿‡äºç®€çŸ­ï¼Œå¯èƒ½ä¸å¤Ÿå®Œæ•´",
                suggestion="è¯·æä¾›æ›´è¯¦ç»†çš„è§£é‡Šæˆ–æ­¥éª¤è¯´æ˜",
                confidence=0.5,
            )
        
        return ReflectionResult(
            should_adjust=False,
            reason="å›ç­”å®Œæ•´ä¸”ç›¸å…³",
            suggestion="å¯ä»¥è¿”å›ç»™ç”¨æˆ·",
            confidence=0.85,
        )
    
    def get_strategy_summary(self, state: StrategyState) -> dict[str, Any]:
        """Get summary of strategy state for debugging.
        
        Args:
            state: Strategy state to summarize
            
        Returns:
            Dictionary with strategy summary
        """
        return {
            "consecutive_failures": state.consecutive_failures,
            "same_tool_repeated": state.same_tool_repeated,
            "last_tool_name": state.last_tool_name,
            "adjustment_count": state.adjustment_count,
            "failed_tool_patterns": list(state.failed_tool_patterns),
            "reflection_count": len(state.reflection_history),
            "tool_execution_count": len(state.tool_execution_history),
            "success_rate": (
                sum(1 for r in state.tool_execution_history if r.success) / len(state.tool_execution_history)
                if state.tool_execution_history else 0
            ),
        }
    
    def _is_empty_result_normal(self, tool_name: str) -> bool:
        """Check if empty result is normal for this tool.
        
        Some tools legitimately return empty results in certain cases.
        
        Args:
            tool_name: Name of the tool
            
        Returns:
            True if empty result is expected/acceptable for this tool
        """
        # Tools that can legitimately return empty results
        tools_allowing_empty = {
            "list_dir",           # Empty directory is valid
            "search_files",       # No matches found is valid
            "aliyun_web_search",  # No search results is valid
            "web_search",         # No search results is valid
            "fetch_web_content",  # Empty page or 404 is possible
        }
        return tool_name in tools_allowing_empty
    
    def _generate_failure_analysis(
        self,
        state: StrategyState,
        iterations: int,
    ) -> dict[str, Any]:
        """Generate failure analysis when max iterations is reached.
        
        Args:
            state: Strategy state
            iterations: Number of iterations executed
            
        Returns:
            Dictionary with failure analysis
        """
        analysis = {
            "primary_reason": "",
            "contributing_factors": [],
            "recommendation": "",
            "suggested_user_action": "",
        }
        
        # Determine primary reason
        if state.consecutive_failures >= self.MAX_CONSECUTIVE_FAILURES:
            analysis["primary_reason"] = "è¿ç»­å¤šæ¬¡å·¥å…·æ‰§è¡Œå¤±è´¥"
            analysis["contributing_factors"].append(f"å·¥å…·å¤±è´¥æ¨¡å¼ï¼š{list(state.failed_tool_patterns)}")
        elif state.same_tool_repeated >= 2:  # P4-2: é™ä½é˜ˆå€¼åˆ° 2 æ¬¡ï¼Œæ›´æ—©å‘ç°å¾ªç¯
            analysis["primary_reason"] = "æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨å¾ªç¯"
            analysis["contributing_factors"].append(f"é‡å¤å·¥å…·ï¼š{state.last_tool_name} (å·²è°ƒç”¨ {state.same_tool_repeated} æ¬¡)")
        elif state.adjustment_count >= self.MAX_ADJUSTMENTS:
            analysis["primary_reason"] = "ç­–ç•¥è°ƒæ•´æ¬¡æ•°è¿‡å¤šï¼Œä»»åŠ¡å¤æ‚åº¦å¯èƒ½è¶…å‡ºå½“å‰èƒ½åŠ›"
        elif iterations >= self.max_iterations:
            analysis["primary_reason"] = "ä»»åŠ¡è¿‡äºå¤æ‚ï¼Œéœ€è¦æ›´å¤šè¿­ä»£æ¬¡æ•°"
        else:
            analysis["primary_reason"] = "æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆçš„ä»»åŠ¡è§£å†³æ–¹æ¡ˆ"
        
        # Add execution stats
        if state.tool_execution_history:
            success_count = sum(1 for r in state.tool_execution_history if r.success)
            total_count = len(state.tool_execution_history)
            analysis["contributing_factors"].append(
                f"å·¥å…·æ‰§è¡ŒæˆåŠŸç‡: {success_count}/{total_count} ({success_count/total_count*100:.1f}%)"
            )
        
        # Generate recommendations
        if state.failed_tool_patterns:
            analysis["recommendation"] = (
                "å»ºè®®æ£€æŸ¥å¤±è´¥å·¥å…·çš„é…ç½®å’Œå‚æ•°ï¼Œæˆ–å°è¯•ä½¿ç”¨æ›¿ä»£å·¥å…·å®Œæˆä»»åŠ¡ã€‚"
            )
        elif state.same_tool_repeated >= self.MAX_SAME_TOOL_REPEATS:
            analysis["recommendation"] = (
                "å»ºè®®é‡æ–°è¯„ä¼°ä»»åŠ¡ç­–ç•¥ï¼Œå°è¯•ä¸åŒçš„æ–¹æ³•ç»„åˆï¼Œé¿å…é‡å¤ç›¸åŒçš„æ“ä½œã€‚"
            )
        else:
            analysis["recommendation"] = (
                "å»ºè®®å°†ä»»åŠ¡åˆ†è§£ä¸ºæ›´å°çš„å­ä»»åŠ¡ï¼Œæˆ–å‘ç”¨æˆ·å¯»æ±‚æ›´æ˜ç¡®çš„æŒ‡å¯¼ã€‚"
            )
        
        analysis["suggested_user_action"] = (
            "æ‚¨å¯ä»¥å°è¯•ï¼š1) ç®€åŒ–ä»»åŠ¡æè¿°ï¼›2) æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›"
            "3) å°†ä»»åŠ¡æ‹†åˆ†ä¸ºå¤šä¸ªæ­¥éª¤ï¼›4) æ£€æŸ¥ç›¸å…³å·¥å…·å’Œèµ„æºçš„å¯ç”¨æ€§ã€‚"
        )
        
        return analysis
    
    def _should_reflect_on_result(self, result: ToolResult) -> tuple[bool, ReflectionType, str]:
        """æ£€æŸ¥å·¥å…·ç»“æœæ˜¯å¦éœ€è¦åæ€ï¼ˆåœºæ™¯1ï¼šç»“æœå¼‚å¸¸ï¼‰
        
        Args:
            result: å·¥å…·æ‰§è¡Œç»“æœ
            
        Returns:
            (æ˜¯å¦éœ€è¦åæ€, åæ€ç±»å‹, åŸå› )
        """
        # APIé”™è¯¯
        if not result.success and result.error:
            return True, ReflectionType.ERROR_DRIVEN, f"å·¥å…·æ‰§è¡Œå¤±è´¥: {result.error}"
        
        # ç»“æœä¸ºç©º
        if result.success and (not result.output or len(result.output.strip()) == 0):
            return True, ReflectionType.ERROR_DRIVEN, "å·¥å…·è¿”å›ç©ºç»“æœ"
        
        # æ ¼å¼ä¸ç¬¦åˆschemaï¼ˆæ£€æŸ¥ç‰¹å®šæ¨¡å¼ï¼‰
        if result.success and result.metadata.get("format_error"):
            return True, ReflectionType.ERROR_DRIVEN, "è¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ"
        
        # ç½®ä¿¡åº¦ä½ï¼ˆå¦‚æœæœ‰confidenceå­—æ®µï¼‰
        confidence = result.metadata.get("confidence", 1.0)
        if confidence < 0.5:
            return True, ReflectionType.ERROR_DRIVEN, f"ç»“æœç½®ä¿¡åº¦è¿‡ä½ ({confidence:.2f})"
        
        return False, ReflectionType.TOOL_RESULT, ""
    
    def _is_high_risk_action(self, tool_name: str, arguments: dict) -> tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜é£é™©æ“ä½œï¼ˆåœºæ™¯3ï¼šPre-Actionåæ€ï¼‰
        
        Args:
            tool_name: å·¥å…·åç§°
            arguments: å·¥å…·å‚æ•°
            
        Returns:
            (æ˜¯å¦é«˜é£é™©, é£é™©æè¿°)
        """
        # é«˜é£é™©å·¥å…·åˆ—è¡¨
        HIGH_RISK_TOOLS = {
            "delete_file": "åˆ é™¤æ–‡ä»¶",
            "delete_directory": "åˆ é™¤ç›®å½•",
            "write_file": "å†™å…¥æ–‡ä»¶",
            "overwrite_file": "è¦†ç›–æ–‡ä»¶",
            "send_email": "å‘é€é‚®ä»¶",
            "execute_code": "æ‰§è¡Œä»£ç ",
            "run_shell": "æ‰§è¡ŒShellå‘½ä»¤",
            "paid_api_call": "è°ƒç”¨ä»˜è´¹API",
            "database_delete": "åˆ é™¤æ•°æ®åº“è®°å½•",
            "api_request": "APIè¯·æ±‚"  # æŸäº›APIå¯èƒ½æœ‰å‰¯ä½œç”¨
        }
        
        if tool_name in HIGH_RISK_TOOLS:
            risk_desc = HIGH_RISK_TOOLS[tool_name]
            # æ£€æŸ¥ç‰¹å®šé«˜é£é™©å‚æ•°
            if tool_name == "write_file" and arguments.get("overwrite"):
                return True, f"{risk_desc}ï¼ˆè¦†ç›–æ¨¡å¼ï¼‰"
            if tool_name == "run_shell" and any(cmd in str(arguments) for cmd in ["rm", "del", "format"]):
                return True, f"{risk_desc}ï¼ˆå±é™©å‘½ä»¤ï¼‰"
            return True, risk_desc
        
        return False, ""
    
    def _should_checkpoint_reflect(self, iteration: int, strategy_state: StrategyState) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é˜¶æ®µæ€§åæ€ï¼ˆåœºæ™¯2ï¼šCheckpointï¼‰
        
        Args:
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            strategy_state: ç­–ç•¥çŠ¶æ€
            
        Returns:
            æ˜¯å¦éœ€è¦åæ€
        """
        # æ¯å®Œæˆä¸€å®šæ•°é‡çš„æˆåŠŸå·¥å…·è°ƒç”¨ååæ€
        if strategy_state.tool_execution_history:
            success_count = sum(1 for r in strategy_state.tool_execution_history if r.success)
            # æ¯3ä¸ªæˆåŠŸå·¥å…·ååæ€ä¸€æ¬¡
            if success_count > 0 and success_count % 3 == 0:
                return True
        
        return False
    
    def _should_long_task_reflect(self, iteration: int, max_iterations: int, plan_state: Any = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦é•¿ä»»åŠ¡èŠ‚å¥åæ€ï¼ˆåœºæ™¯ 5ï¼šLong Taskï¼‰
            
        Args:
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            plan_state: Plan çŠ¶æ€å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºæ›´ç²¾ç¡®çš„è¿›åº¦æ£€æµ‹ï¼‰
                
        Returns:
            æ˜¯å¦éœ€è¦åæ€
        """
        # P4-3 NEW: å¦‚æœæœ‰ plan_stateï¼Œæ·»åŠ é¢å¤–çš„è¿›åº¦æ£€æŸ¥
        if plan_state and hasattr(plan_state, 'current_step'):
            # æ¯ 2 æ¬¡è¿­ä»£æ£€æŸ¥ä¸€æ¬¡è¿›å±•
            if iteration % 2 == 0 and iteration > 0:
                # ä½¿ç”¨å®ä¾‹å˜é‡è·Ÿè¸ªä¸Šæ¬¡çš„ step
                if not hasattr(self, '_last_step_snapshot'):
                    self._last_step_snapshot = plan_state.current_step
                elif self._last_step_snapshot == plan_state.current_step:
                    # 2 æ¬¡è¿­ä»£åä»åœ¨åŒä¸€ stepï¼Œéœ€è¦åæ€
                    logger.warning(
                        "Slow progress detected",
                        extra={
                            "iteration": iteration,
                            "current_step": plan_state.current_step,
                            "last_step": self._last_step_snapshot,
                            "tool_execution_count": getattr(plan_state, 'tool_execution_count', 0),
                        }
                    )
                    return True
                else:
                    # æ›´æ–°å¿«ç…§
                    self._last_step_snapshot = plan_state.current_step
            
        # åŸæœ‰é€»è¾‘ï¼šåœ¨ä»»åŠ¡ä¸­æœŸï¼ˆ1/3 å’Œ 2/3 å¤„ï¼‰è¿›è¡Œåæ€ï¼Œé˜²æ­¢è·‘å
        checkpoints = [max_iterations // 3, (max_iterations * 2) // 3]
        return iteration in checkpoints
        
    def _should_step_stuck_reflect(
        self,
        iteration: int,
        plan_state: Any,
        strategy_state: StrategyState,
    ) -> tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦å›  Step åœæ»éœ€è¦åæ€ï¼ˆæ–°å¢åœºæ™¯ï¼šStep Stuckï¼‰
            
        è§¦å‘æ¡ä»¶:
        1. current_step è¿ç»­ 3 æ¬¡è¿­ä»£æœªå˜åŒ–
        2. tool_execution_count > 3 ä¸”ä»åœ¨åŒä¸€ Step
        3. åŒä¸€å·¥å…·é‡å¤è°ƒç”¨ >= 2 æ¬¡
            
        Args:
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            plan_state: Plan çŠ¶æ€å¯¹è±¡
            strategy_state: ç­–ç•¥çŠ¶æ€
                
        Returns:
            tuple[bool, str]: (æ˜¯å¦éœ€è¦åæ€ï¼ŒåŸå› æè¿°)
        """
        if not plan_state:
            return False, ""
            
        # æ£€æŸ¥ 1: tool_execution_count è¿‡é«˜ä½†ä»åœ¨åŒä¸€ Step
        if hasattr(plan_state, 'tool_execution_count') and hasattr(plan_state, 'current_step'):
            if plan_state.tool_execution_count > 3:
                # æ£€æµ‹æ˜¯å¦åœ¨åŒä¸€ä¸ª step ä¸Šæ‰§è¡Œäº†è¿‡å¤šå·¥å…·
                reason = f"Step åœæ»æ£€æµ‹ï¼šcurrent_step={plan_state.current_step}, tool_execution_count={plan_state.tool_execution_count}"
                return True, reason
            
        # æ£€æŸ¥ 2: åŒä¸€å·¥å…·é‡å¤è°ƒç”¨ >= 2 æ¬¡
        if hasattr(strategy_state, 'same_tool_repeated') and strategy_state.same_tool_repeated >= 2:
            reason = f"å·¥å…·é‡å¤æ£€æµ‹ï¼š{strategy_state.last_tool_name} å·²è°ƒç”¨ {strategy_state.same_tool_repeated} æ¬¡"
            return True, reason
            
        return False, ""
